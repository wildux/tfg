\section{Programa principal en python}
	Per facilitar la utilització del codi en possibles adaptacions o millores futures, s'ha decidit implementar una petita biblioteca en Python amb totes les funcions necessaries. El programa principal farà ús
	d'aquesta biblioteca, que permetrà:

	\begin{enumerate}
		\item{Pre-processar les imatges}
		\item{Seleccionar la regió d'interès}
		\item{Obtenir els keypoints}
		\item{Extreure les característiques}
		\item{Fer matching de característiques}
		\item{Homografia: Obtenir la regió/punt demanat}
		\item{Obtenir l'angle de rotació pel robot}
	\end{enumerate}

	\subsection{Pre-processat de les imatges}
		S'han provat diverses tècniques de pre-processat com ara filtres gaussians o canvis en el contrast, pero els resultats obtinguts no han sigut satisfactoris i finalment s'ha optat per deixar
		les imatges tal com són.\\\\
		Simplement es transforma la imatge a escala de grisos per poder treballar amb tots els algorismes de visió i es redimensiona per agilitzar la selecció de keypoints, extracció
		de característiques i matching.\\
		\begin{python}
def prep(image):
	img = cv2.resize(image, (0,0), fx=0.3, fy=0.3)
	return img, cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
		\end{python}
	\subsection{Selecció de la regió d'interès}
		Per tal de poder seleccionar la regió d'interès amb facilitat, s'ha definit la funció \textbf{selectROI(image)}, que permet a l'usuari seleccionar una regió rectangular de la imatge donada
		com a paràmetre. La imatge resultant serà aquesta selecció.\\
		\begin{python}
def click_and_crop(event, x, y, flags, param):
	global refPt, cropping, sel_rect_endpoint, img
 
	if event == cv2.EVENT_LBUTTONDOWN:	# Initial coordinates. Cropping = true
		cropping = True
		refPt = [(x, y)] 
	elif event == cv2.EVENT_LBUTTONUP:	# End coordinates. Cropping = false (done)
		cropping = False
		refPt.append((x, y)) 
		clone = img.copy()
		cv2.rectangle(clone, refPt[0], refPt[1], (0, 255, 255), 2)	# Draw a rectangle (ROI)
		cv2.imshow("image", clone)
	elif event == cv2.EVENT_MOUSEMOVE and cropping:	# Update position (moving rectangle)
		sel_rect_endpoint = [(x, y)]

def selectROI(image):
	global img, refPt, sel_rect_endpoint
	img = image ###
	cv2.namedWindow("image")
	cv2.setMouseCallback("image", click_and_crop)
	cv2.imshow('image', img)

	while True:
		if not cropping:
			sel_rect_endpoint = []
		elif cropping and sel_rect_endpoint:	# Display rectangle (moving)
			clone = img.copy()
			cv2.rectangle(clone, refPt[0], sel_rect_endpoint[0], (0, 255, 0), 1)
			cv2.imshow('image', clone)
		if (cv2.waitKey(1) & 0xFF) == ord("c"):
			break

	cv2.destroyAllWindows()
	if len(refPt) == 2:
		img = img[refPt[0][1]:refPt[1][1], refPt[0][0]:refPt[1][0]]
	return img
		\end{python}
	\subsection{Obtenció de keypoints}
		Per obtenir els punts d'interès d'una imatge, s'utilitzaràn diversos algorismes de visió per computador.\\\\
		La funció point{\_}selection(gray, alg) serà l'encarregada de cridar l'algoritme desitjat segons el que vulgui l'usuari (per defecte s'utilitzarà Harris).
		Els paràmetres necessaris seràn una imatge en escala de grisos i l'algorisme desitjat. La funció retornarà un array amb els keypoints trobats. \\
		\begin{python}
def point_selection(gray, alg):
	kp = []
	...
	return kp
		\end{python}

		\subsubsection{SIFT}
		Una de les opcions serà utilitzar SIFT (DoG). Per fer això simplement cridarem la funció d'OpenCV detect() en la instància de SIFT creada.\\
		\begin{python}
	if alg == _SIFT:
		sift = cv2.xfeatures2d.SIFT_create()
		kp = sift.detect(gray,None)
		\end{python}

		\subsubsection{ORB}
		Per utilitzar ORB també cridarem la funció detect() d'OpenCV, pero en aquest cas haurem de modificar una mica els paràmetres per defecte de la creació de l'objecte ORB, ja que els resultats obtinguts
		en primer moment no eren gaire bons.\\
		\begin{python}
	elif alg == _ORB:
		orb = cv2.ORB_create(nfeatures = 5000, nlevels = 8, edgeThreshold = 8,
		 patchSize = 8, fastThreshold = 5)
		kp = orb.detect(gray,None)
		\end{python}
\newpage
		\subsubsection{Harris}
		En el cas de Harris, el detector no utilitza diferents escales i simplement detecta ``corners'' en la imatge. Per tant, utilitzarem la funció d'OpenCV pyrDown() per reduir
		la mida de la imatge (1/2) i aplicar Harris en diverses escales.\\\\
		Per detectar els keypoints farem servir la funció goodFeaturesToTrack() en comptes del detector de corners de Harris, ja que permet obtenir els punts més facilment i
		també té la opció d'utilitzar Shi-Tomasi si ens interessés.\\
		\begin{python}
	elif alg == _HARRIS:
		G = gray.copy()
		for i in range(5):
			if i != 0:
				G = cv2.pyrDown(G)
			scale = 2**(i)
			corners = cv2.goodFeaturesToTrack(image=G,maxCorners=1000,
				qualityLevel=0.01,minDistance=scale,useHarrisDetector=1, k=0.04)
			corners = np.int0(corners)
			for corner in corners:
				x,y = corner.ravel()
				k = cv2.KeyPoint(x*scale, y*scale, scale)
				kp.append(k)
		\end{python}

		\subsubsection{MSER}
		Per últim, tindrem la opció d'utilitzar MSER. Tal com fem amb SIFT i ORB, també farem servir la funció detect().\\
		\begin{python}
	elif alg == _MSER:
		mser = cv2.MSER_create()
		kp = mser.detect(gray,None)
		\end{python}
\newpage
	\subsection{Extracció de característiques}
		De la mateixa manera en que podem aconseguir keypoints, OpenCV també disposa de diversos algorismes d'extracció de característiques a partir dels keypoints d'una imatge.\\\\
		La funció creada feature{\_}detection(image, kp, alg) serà l'encarregada d'extreure les característiques utilitzant algun dels algorismes disponibles. Els paràmetres necessaris de la funció són:
		la imatge en escala de grisos, l'array de keypoints obtinguts i l'algorisme desitjat. Retornarà tant els descriptors com els keypoints.\\

		\begin{python}
def feature_extraction(image, kp, alg):
	des = []
	...
	return kp, des
		\end{python}
		\ \\Els algorismes que podrà escollir l'usuari són:
		\begin{itemize}
			\item{SIFT}
			\item{SURF}
			\item{ORB}
			\item{BRISK}
			\item{LATCH}
			\item{DAISY}
		\end{itemize}
	\ \\S'utilitzaran els algorismes ja implementats en la biblioteca OpenCV de la següent manera:\\

		\begin{python}
	elif alg == _LATCH:
		latch = cv2.xfeatures2d.LATCH_create()
		kp, des = latch.compute(image, kp)
		\end{python}

\newpage
	\subsection{Matching i homografia}
La funció retornarà les coordenades de destí (x,y) i una imatge amb els matches i la regió de destí pintats.\\
		\begin{python}
def matching(img1, img2, des1, des2, kp1, kp2, fe):
	draw_params = dict(matchColor = (0,255,0), singlePointColor = None,
					matchesMask = matchesMask, flags = 2)
	return x, y, cv2.drawMatches(img1,kp1,img2,kp2,good,None,**draw_params)
		\end{python}

		\subsubsection{Matching}
		En el cas dels descriptors binaris, utilitzarem la distancia Hamming, mentre que per la resta s'utilitzarà... En els dos casos, s'utilitzarà la funció BFMatcher(), que aplicarà el matching
		per força bruta.\\
		\begin{python}
	if fe == _LATCH or fe == _ORB or fe == _BRISK:
		bf = cv2.BFMatcher(cv2.NORM_HAMMING)
	else:
		bf = cv2.BFMatcher()

	matches = bf.knnMatch(des1, des2, k=2)
		\end{python}

		\subsubsection{Ratio test}
Un cop obtinguts els "matches", aplicarem el ratio test per descartar alguns "matches".\\
		\begin{python}
	good = []
	for m,n in matches:
		if m.distance < 0.75*n.distance:
			good.append(m)
		\end{python}

\newpage
		\subsubsection{Homografia}
Si hi ha prou coincidencies (considerem acceptable com a mínim 10), es buscarà la homografia i es pintarà la regió trobada en la imatge. S'aplicarà Ransac. \\
		\begin{python}
	if len(good) >= MIN_MATCH_COUNT:
		src_pts = np.float32([ kp1[m.queryIdx].pt for m in good ]).reshape(-1,1,2)
		dst_pts = np.float32([ kp2[m.trainIdx].pt for m in good ]).reshape(-1,1,2)
		M, mask = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC, 5.0)
		matchesMask = mask.ravel().tolist()
		h,w,_ = img1.shape
		pts = np.float32([ [0,0],[0,h-1],[w-1,h-1],[w-1,0] ]).reshape(-1,1,2)
		dst = cv2.perspectiveTransform(pts,M)
		img2 = cv2.polylines(img2,[np.int32(dst)],True,255,3, cv2.LINE_AA)
		\end{python}

		\subsubsection{Coordenades de destí}
Per obtenir el punt on volem que es dirigeixi el robot agafarem els valors de dst i retornarem el punt mig.\\
		\begin{python}
		x1, y1 = np.int32(dst)[0].ravel()
		x2, y2 = np.int32(dst)[1].ravel()
		x3, y3 = np.int32(dst)[2].ravel()
		x4, y4 = np.int32(dst)[3].ravel()
		x = (x1+x2+x3+x4)/4
		y = (y1+y2+y3+y4)/4
		\end{python}

	\subsection{Angle de gir}
		Un cop obtingudes les coordenades de la imatge, s'obtindrà l'angle de gir necessari pel robot a partir de l'angle de visió de la càmera i les dimensions de la imatge.\\
		\begin{python}
def getAngle(aV, w, x):
	if x == -1:
		return 0
	else:
		return (aV*x / w) - (aV/2)
		\end{python}

\section{Aplicació web}
	L'aplicació web s'ha desenvolupat amb Flask (Python).
	\subsection{Pujar imatge al servidor}
		Per poder utilitzar la càmera del mòbil s'ha hagut d'afegir el permís necessari.\\\\
		Com que Android ja disposa d'una Activitat que ens permet llançar la càmera desde una altre aplicació, nomès ha sigut necessari fer l'Intent corresponent.\\
		\begin{python}
app.config['UPLOAD_FOLDER'] = 'static/'
app.config['ALLOWED_EXTENSIONS'] = set(['png', 'jpg', 'jpeg', 'gif'])

# For a given file, return whether it's an allowed type or not
def allowed_file(filename):
	return '.' in filename and 
		filename.rsplit('.', 1)[1] in app.config['ALLOWED_EXTENSIONS']

@app.route("/update")
def update():
	return render_template('upload.html')

@app.route('/upload', methods=['POST'])
def upload():
	file = request.files['file']
	if file and allowed_file(file.filename):
		filename = secure_filename(file.filename)
		file.save(os.path.join(app.config['UPLOAD_FOLDER'], filename))
		return redirect(url_for('uploaded_file', filename=filename))

@app.route('/uploads/<filename>')
def uploaded_file(filename):
	return send_from_directory(app.config['UPLOAD_FOLDER'], filename)
		\end{python}

		\begin{txt}
<form action="upload" method="post" 
	enctype="multipart/form-data">
	<div class="col-lg-6 col-sm-6 col-12">
		<label class="input-group-btn">
			<span class="btn btn-primary">
				Browse&hellip; <input type="file" 
					name="file" style="display: none;">
			</span>
		</label>
	</div>
	<div class="col-lg-6 col-sm-6 col-12">
		<input class="btn btn-primary" type="submit"
			value="Pujar imatge">
	</div>
</form>
		\end{txt}

	\subsection{Selecció de la regió d'interès}
		Per poder utilitzar la càmera del mòbil s'ha hagut d'afegir el permís necessari.\\\\
		Com que Android ja disposa d'una Activitat que ens permet llançar la càmera desde una altre aplicació, nomès ha sigut necessari fer l'Intent corresponent.\\
		\begin{txt}
<form action="send" method="post" >
	<input type="hidden" name="x1" value="" />
	<input type="hidden" name="y1" value="" />
	<input type="hidden" name="x2" value="" />
	<input type="hidden" name="y2" value="" />
	<input type="hidden" name="width" value="" />
	<input type="hidden" name="height" value="" />
	<input class="btn btn-primary" type="submit"
		value="Send" />
</form>

<script type="text/javascript">
	jQuery(document).ready(function () {
		jQuery('img#scene').imgAreaSelect({
			handles: true,
			persistent: true,
			imageHeight: 2448,
			imageWidth: 3264,
			x1: 100, y1: 100, x2: 300, y2: 300,
			onInit: function ( image, selected) {
				jQuery('input[name=x1]').val(selected.x1);
				jQuery('input[name=y1]').val(selected.y1);
				jQuery('input[name=x2]').val(selected.x2);
				jQuery('input[name=y2]').val(selected.y2);
				jQuery('input[name=width]')
					.val(selected.width);
				jQuery('input[name=height]')
					.val(selected.height);
			},
			onSelectEnd: function ( image, selected) {
				jQuery('input[name=x1]').val(selected.x1);
				jQuery('input[name=y1]').val(selected.y1);
				jQuery('input[name=x2]').val(selected.x2);
				jQuery('input[name=y2]').val(selected.y2);
				jQuery('input[name=width]')
					.val(selected.width);
				jQuery('input[name=height]')
					.val(selected.height);
			}
		});
	});
 </script>
		\end{txt}

		\begin{python}
@app.route("/")
def index():
	return render_template('index.html')

@app.route('/send', methods=['POST'])
def send():
	x1 = int(request.form['x1'])
	x2 = int(request.form['x2'])
	y1 = int(request.form['y1'])
	y2 = int(request.form['y2'])

	img = cv2.imread(imgPath)
	img = img[x1:x2, y1:y2]
	imgRobot = cv2.imread(imgRobotPath)
	imgRes, x, y = vc.getResult(img, imgRobot, vc._SIFT)
	cv2.imwrite("static/result.png", imgRes)

	return render_template('result.html', x=x, y=y)
		\end{python}


\section{Aplicació mòbil}
	L'aplicació mòbil s'ha desenvolupat amb Android Studio i el llenguatge de programació Java (amb l'SDK d'Android).
	\subsection{Capturar imatge}
		Per poder utilitzar la càmera del mòbil s'ha hagut d'afegir el permís necessari.\\\\
		Com que Android ja disposa d'una Activitat que ens permet llançar la càmera desde una altre aplicació, nomès ha sigut necessari fer l'Intent corresponent.\\
		\begin{java}
class MyClass(Yourclass):
  def __init__(self, my, yours):
    String = "String"
    String2 = '5 1 2 3 4'
    print String2
import numpy as np #Comment1
  # Comment2
		\end{java}
	\subsection{Selecció d'imatges de la galeria}
		Per poder utilitzar la càmera del mòbil s'ha hagut d'afegir el permís necessari.\\\\
		Com que Android ja disposa d'una Activitat que ens permet llançar la càmera desde una altre aplicació, nomès ha sigut necessari fer l'Intent corresponent.\\
		\begin{java}
class MyClass(Yourclass):
  def __init__(self, my, yours):
    String = "String"
    String2 = '5 1 2 3 4'
    print String2
import numpy as np #Comment1
  # Comment2
		\end{java}
	\subsection{Enviament de dades al servidor}
		Per poder enviar dades al servidor, serà necessari l'ús d'una connexió a Internet. Per tant, s'haurà d'habilitar el permís necessari.\\\\
		Com que Android ja disposa d'una Activitat que ens permet llançar la càmera desde una altre aplicació, nomès ha sigut necessari fer l'Intent corresponent.\\
		\begin{java}
class MyClass(Yourclass):
  def __init__(self, my, yours):
    String = "String"
    String2 = '5 1 2 3 4'
    print String2
import numpy as np #Comment1
  # Comment2
		\end{java}
