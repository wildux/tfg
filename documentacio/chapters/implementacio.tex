\label{sec:Implementacio}

\section{Programa principal en Python}
	Per facilitar la utilització del codi en possibles adaptacions o millores futures, s'ha decidit implementar una petita biblioteca\footnote{Codi disponible a \url{https://github.com/wildux/tfg}}
	en Python amb totes les funcions necessàries. El programa principal farà ús d'aquesta biblioteca, que permetrà:

	\begin{enumerate}
		\item{Preprocessar les imatges}
		\item{Seleccionar la regió d'interès}
		\item{Obtenir els \textit{keypoints}}
		\item{Extreure les característiques}
		\item{Fer \textit{matching} de característiques}
		\item{Homografia: Obtenir la regió/punt demanat}
		%\item{Obtenir l'angle de rotació pel robot}
	\end{enumerate}

	\subsection{Preprocessat de les imatges}
		S'han provat diverses tècniques de preprocessat com ara filtres gaussians o canvis en el contrast, però els resultats obtinguts no han sigut satisfactoris i finalment s'ha optat per deixar
		les imatges tal com són.\\\\
		Simplement es transforma la imatge a escala de grisos per poder treballar amb tots els algorismes de visió i es redimensiona per agilitzar la selecció de \textit{keypoints}, extracció
		de característiques i \textit{matching}.\\
		\begin{python}
def prep(image):
	img = cv2.resize(image, (0,0), fx=0.5, fy=0.5)
	return img, cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
		\end{python}
	\subsection{Selecció de la regió d'interès}
		Per tal de poder seleccionar la regió d'interès amb facilitat, s'ha definit la funció \textbf{selectROI(image)}, que permet a l'usuari seleccionar una regió rectangular de la imatge donada
		com a paràmetre. La imatge resultant serà aquesta selecció.\\
		\begin{python}
def click_and_crop(event, x, y, flags, param):
	global refPt, cropping, sel_rect_endpoint, img
 
	if event == cv2.EVENT_LBUTTONDOWN:	# Initial coordinates. Cropping = true
		cropping = True
		refPt = [(x, y)] 
	elif event == cv2.EVENT_LBUTTONUP:	# End coordinates. Cropping = false (done)
		cropping = False
		refPt.append((x, y)) 
		clone = img.copy()
		cv2.rectangle(clone, refPt[0], refPt[1], (0, 255, 255), 2)	# Draw a rectangle (ROI)
		cv2.imshow("image", clone)
	elif event == cv2.EVENT_MOUSEMOVE and cropping:	# Update position (moving rectangle)
		sel_rect_endpoint = [(x, y)]

def selectROI(image):
	global img, refPt, sel_rect_endpoint
	img = image
	cv2.namedWindow("image")
	cv2.setMouseCallback("image", click_and_crop)
	cv2.imshow('image', img)

	while True:
		if not cropping:
			sel_rect_endpoint = []
		elif cropping and sel_rect_endpoint:	# Display rectangle (moving)
			clone = img.copy()
			cv2.rectangle(clone, refPt[0], sel_rect_endpoint[0], (0, 255, 0), 1)
			cv2.imshow('image', clone)
		if (cv2.waitKey(1) & 0xFF) == ord("c"):
			break

	cv2.destroyAllWindows()
	if len(refPt) == 2:
		img = img[refPt[0][1]:refPt[1][1], refPt[0][0]:refPt[1][0]]
	return img
		\end{python}
	\subsection{Obtenció de \textit{keypoints}}
		Per obtenir els punts d'interès d'una imatge, s'utilitzaran diversos algorismes de visió per computador.\\\\
		La funció \textbf{point{\_}selection(gray, alg)} serà l'encarregada de cridar l'algoritme escollit per l'usuari (per defecte s'utilitzarà Harris).
		Els paràmetres necessaris seran una imatge en escala de grisos i l'algorisme desitjat. La funció retornarà un \textit{array} amb els \textit{keypoints} trobats. \\
		\begin{python}
def point_selection(gray, alg):
	kp = []
	...
	return kp
		\end{python}

		\subsubsection{SIFT}
		Una de les opcions serà utilitzar SIFT (DoG). Per fer això, simplement cridarem la funció d'OpenCV \textbf{detect()} en la instància de SIFT creada. Modificarem el paràmetre \textit{sigma} perquè funcioni millor
		amb les imatges utilitzades.\\
		\begin{python}
	if alg == _SIFT:
		sift = cv2.xfeatures2d.SIFT_create(sigma=1.4)
		kp = sift.detect(gray,None)
		\end{python}

		\subsubsection{ORB}
		Per utilitzar ORB també cridarem la funció \textbf{detect()} d'OpenCV, però en aquest cas haurem de modificar una mica els paràmetres per defecte de la creació de l'objecte ORB, ja que els resultats obtinguts
		en primer moment no eren gaire bons.\\
		\begin{python}
	elif alg == _ORB:
		orb = cv2.ORB_create(nfeatures = 2500, nlevels = 8, edgeThreshold = 8,
		 patchSize = 8, fastThreshold = 5)
		kp = orb.detect(gray,None)
		\end{python}
\newpage
		\subsubsection{Harris}
		En el cas de Harris, el detector no utilitza diferents escales i simplement detecta \textit{corners} en la imatge. Per tant, utilitzarem la funció d'OpenCV \textbf{pyrDown()} per reduir
		la mida de la imatge (\sfrac{1}{2}) i aplicar Harris en diverses escales.\\\\
		Per detectar els \textit{keypoints} farem servir la funció \textbf{goodFeaturesToTrack()} en comptes del detector de corners de Harris, ja que permet obtenir els punts més fàcilment i
		també té l'opció d'utilitzar Shi-Tomasi si ens interessés.\\
		\begin{python}
	elif alg == _HARRIS:
		G = gray.copy()
		for i in range(5):
			if i != 0:
				G = cv2.pyrDown(G)
			scale = 2**(i)
			corners = cv2.goodFeaturesToTrack(image=G,maxCorners=2000,
				qualityLevel=0.05,minDistance=4,useHarrisDetector=1, k=0.04)
			corners = np.int0(corners)
			for corner in corners:
				x,y = corner.ravel()
				k = cv2.KeyPoint(x*scale, y*scale, scale*3)
				kp.append(k)
		\end{python}

		\subsubsection{MSER}
		Per últim, tindrem l'opció d'utilitzar MSER. Tal com fem amb SIFT i ORB, també farem servir la funció \textbf{detect()}.\\
		\begin{python}
	elif alg == _MSER:
		mser = cv2.MSER_create()
		kp = mser.detect(gray,None)
		\end{python}
\newpage
	\subsection{Extracció de característiques}
		De la mateixa manera en què podem aconseguir \textit{keypoints}, OpenCV també disposa de diversos algorismes d'extracció de característiques a partir dels \textit{keypoints} d'una imatge.\\\\
		La funció creada \textbf{feature{\_}detection(image, kp, alg)} serà l'encarregada d'extreure les característiques utilitzant algun dels algorismes disponibles. Els paràmetres necessaris de la funció són:
		la imatge en escala de grisos, l'\textit{array} de \textit{keypoints} obtinguts i l'algorisme desitjat. Retornarà tant els descriptors com els \textit{keypoints}.\\

		\begin{python}
def feature_extraction(image, kp, alg):
	des = []
	...
	return kp, des
		\end{python}
		\ \\Els algorismes que podrà escollir l'usuari són:
		\begin{multicols}{3} 
			\begin{itemize}
				\item{SIFT}
				\item{SURF}
				\item{ORB}
				\item{BRISK}
				\item{LATCH}
				\item{DAISY}
			\end{itemize}
		\end{multicols}
	\ \\S'utilitzaran els algorismes ja implementats en la biblioteca OpenCV de la següent manera:\\

		\begin{python}
	elif alg == _LATCH:
		latch = cv2.xfeatures2d.LATCH_create()
		kp, des = latch.compute(image, kp)
		\end{python}

\newpage
	\subsection{\textit{Matching} i homografia}
La funció \textbf{matching()} retornarà els aparellaments trobats.\\
		\begin{python}
def matching(img1, img2, des1, des2, kp1, kp2, fe):
	draw_params = dict(matchColor = (0,255,0), singlePointColor = None,
					matchesMask = matchesMask, flags = 2)
	return x, y, cv2.drawMatches(img1,kp1,img2,kp2,good,None,**draw_params)
		\end{python}

		\subsubsection{\textit{Matching}}
		En el cas dels descriptors binaris, utilitzarem la distància de Hamming, mentre que per la resta s'utilitzarà l'euclidiana. En els dos casos, s'utilitzarà la funció \textbf{BFMatcher()}, que aplicarà el
		\textit{matching} per força bruta.\\
		\begin{python}
	if fe == _LATCH or fe == _ORB or fe == _BRISK:
		bf = cv2.BFMatcher(cv2.NORM_HAMMING)
	else:
		bf = cv2.BFMatcher()

	matches = bf.knnMatch(des1, des2, k=2)
		\end{python}

		\subsubsection{\textit{Ratio test}}
Un cop obtinguts els aparellaments, aplicarem el \textit{ratio test} per descartar-ne alguns.\\
		\begin{python}
	good = []
	for m,n in matches:
		if m.distance < 0.75*n.distance:
			good.append(m)
		\end{python}

\newpage
		\subsubsection{Homografia}
La funció \textbf{homography()} retornarà les coordenades de destí (x,y) i una imatge amb els aparellaments i la regió de destí pintats.\\
		\begin{python}
homography(img1, img2, des1, des2, kp1, kp2, good):
	...
	draw_params = dict(matchColor = (0,255,0), singlePointColor = None,
		matchesMask = matchesMask, flags = 2)
	img3 = cv2.drawMatches(img1,kp1,img2C,kp2,good,None,**draw_params)
	return x, y, img3
		\end{python}
\noindent
\\{}
Si hi ha prou coincidències (considerem acceptable com a mínim 10), es buscarà l'homografia i es pintarà la regió trobada en la imatge. S'aplicarà RANSAC. \\
		\begin{python}
	if len(good) >= MIN_MATCH_COUNT:
		src_pts = np.float32([ kp1[m.queryIdx].pt for m in good ]).reshape(-1,1,2)
		dst_pts = np.float32([ kp2[m.trainIdx].pt for m in good ]).reshape(-1,1,2)
		M, mask = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC, 5.0)
		matchesMask = mask.ravel().tolist()
		h,w,_ = img1.shape
		pts = np.float32([ [0,0],[0,h-1],[w-1,h-1],[w-1,0] ]).reshape(-1,1,2)
		dst = cv2.perspectiveTransform(pts,M)
		img2C = cv2.polylines(img2C,[np.int32(dst)],True,255,3, cv2.LINE_AA)
		\end{python}

		\subsubsection{Coordenades de destí}
Per obtenir el punt on volem que es dirigeixi el robot agafarem els valors de ``dst'' i retornarem el punt mitjà.\\
		\begin{python}
		x1, y1 = np.int32(dst)[0].ravel()
		x2, y2 = np.int32(dst)[1].ravel()
		x3, y3 = np.int32(dst)[2].ravel()
		x4, y4 = np.int32(dst)[3].ravel()
		x = (x1+x2+x3+x4)/4
		y = (y1+y2+y3+y4)/4
		img2C = cv2.circle(img2C,(int(x),int(y)), 5, (0,0,255), -1)
		\end{python}

%	\subsection{Angle de gir}
%		Un cop obtingudes les coordenades de la imatge, s'obtindrà l'angle de gir necessari pel robot a partir de l'angle de visió de la càmera i les dimensions de la imatge.\\
%		\begin{python}
%def getAngle(aV, w, x):
%	if x == -1:
%		return 0
%	else:
%		return (aV*x / w) - (aV/2)
%		\end{python}
\newpage
\section{Aplicació web}
	L'aplicació web s'ha desenvolupat amb Flask (Python).
	\subsection{Pujar imatge al servidor}
		Per poder canviar la imatge que mostrarà el sistema, es defineix la ruta `/update', que ens mostra un formulari per penjar un arxiu (imatge).\\
		\begin{python}
app.config['UPLOAD_FOLDER'] = 'static/'
app.config['ALLOWED_EXTENSIONS'] = set(['png', 'jpg', 'jpeg', 'gif'])


def allowed_file(filename):
	return '.' in filename and 
		filename.rsplit('.', 1)[1] in app.config['ALLOWED_EXTENSIONS']

@app.route("/update")
def update():
	return render_template('upload.html')

@app.route('/upload', methods=['POST'])
def upload():    
	file = request.files['file']
	if file and allowed_file(file.filename):        
		ext = filename.rsplit('.', 1)[1]
		filename = "scene." + ext        
		file.save(os.path.join(app.config['UPLOAD_FOLDER'], filename))
		return redirect(url_for('uploaded_file', filename=filename))

@app.route('/uploads/<filename>')
def uploaded_file(filename):
	return send_from_directory(app.config['UPLOAD_FOLDER'], filename)
		\end{python}

\newpage
\noindent
El formulari de l'arxiu `upload.html' és el següent:\\

		\begin{txt}
<form action="upload" method="post" 
	enctype="multipart/form-data">
	<div class="col-lg-6 col-sm-6 col-12">
		<label class="input-group-btn">
			<span class="btn btn-primary">
				Browse&hellip; <input type="file" 
					name="file" style="display: none;">
			</span>
		</label>
	</div>
	<div class="col-lg-6 col-sm-6 col-12">
		<input class="btn btn-primary" type="submit"
			value="Pujar imatge">
	</div>
</form>
		\end{txt}

	\subsection{Selecció de la regió d'interès}
		Per seleccionar la regió d'interès s'ha utilitzat jQuery i el \textit{plugin} \textbf{imgAreaSelect}, essent també compatible amb dispositius mòbils.\\\\
		En la pàgina principal es mostra una imatge i un botó per acceptar la selecció. El \textit{plugin} imgAreaSelect s'encarrega de permetre a l'usuari fer una selecció i actualitzar els camps
		amagats del formulari.\\
		\begin{txt}
<form action="send" method="post" >
	<input type="hidden" name="x1" value="" />
	<input type="hidden" name="y1" value="" />
	<input type="hidden" name="x2" value="" />
	<input type="hidden" name="y2" value="" />
	<input type="hidden" name="width" value="" />
	<input type="hidden" name="height" value="" />
	<input class="btn btn-primary" style="float:right"
		type="submit" value="Acceptar sel." />
</form>

<script type="text/javascript">
	jQuery(document).ready(function () {
		var img = document.getElementById('scene');
		document.getElementById('width')
			.value = img.clientWidth
		document.getElementById('height')
			.value = img.clientHeight

		jQuery('img#scene').imgAreaSelect({
			handles: true,
			persistent: true,
			x1: 50, y1: 50, x2: 200, y2: 200,
			onInit: function ( image, selected) {
				jQuery('input[name=x1]').val(selected.x1);
				jQuery('input[name=y1]').val(selected.y1);
				jQuery('input[name=x2]').val(selected.x2);
				jQuery('input[name=y2]').val(selected.y2);              
			},
			onSelectEnd: function ( image, selected) {
				jQuery('input[name=x1]').val(selected.x1);
				jQuery('input[name=y1]').val(selected.y1);
				jQuery('input[name=x2]').val(selected.x2);
				jQuery('input[name=y2]').val(selected.y2);                           
			}
		});
	});
 </script>
		\end{txt}
\newpage
\noindent
El formulari ens porta a la ruta `/send', que executarà el codi principal de visió per computador. Finalment podrem veure el resultat obtingut del \textit{matching} a `result.html'.\\

		\begin{python}
imgPath = 'static/scene.jpg'
imgRobotPath = 'static/uni.jpg'

@app.route("/")
def index():
	return render_template('index.html')

@app.route('/send', methods=['POST'])
def send():
	x1 = int(request.form['x1'])
	x2 = int(request.form['x2'])
	y1 = int(request.form['y1'])
	y2 = int(request.form['y2'])
	height = int(request.form['height'])
	width = int(request.form['width'])

	img = cv2.imread(imgPath)
	h,w,_ = img.shape
	scale_h = h/height
	scale_w = w/width
	x1 = int(x1*scale_w)
	x2 = int(x2*scale_w)
	y1 = int(y1*scale_h)
	y2 = int(y2*scale_h)

	img = img[y1:y2, x1:x2]
	imgRobot = cv2.imread(imgRobotPath)
	imgRes, x, y = vc.getResult(img, imgRobot, vc._SIFT)
	resultPath = "static/" + strftime("%Y-%m-%d-%H:%M") + ".png" 
	cv2.imwrite(resultPath, imgRes)

	return render_template('result.html', x=x, y=y, image=resultPath)
		\end{python}

\newpage
\section{Aplicació mòbil}
	L'aplicació mòbil s'està desenvolupant amb Android Studio i el llenguatge de programació Java (amb l'SDK d'Android).
	\subsection{Capturar imatge}
		Per poder utilitzar la càmera del mòbil s'ha hagut d'afegir el permís necessari.\\\\
		Com que Android ja disposa d'una activitat que ens permet llançar la càmera des d'una altra aplicació, només ha sigut necessari fer l'Intent corresponent.\\
		\begin{java}
if (v.getId() == R.id.capture_btn) try {
	Intent captureIntent = new Intent(MediaStore.ACTION_IMAGE_CAPTURE);
	startActivityForResult(captureIntent, CAMERA_CAPTURE);
} catch (ActivityNotFoundException anfe) {...}
		\end{java}
	\subsection{Selecció d'imatges de la galeria}
		Android també disposa d'una activitat que ens permet obrir la galeria des d'una altra aplicació, així que s'ha seguit el mateix procés que per obrir la càmera.\\
		\begin{java}
else if (v.getId() == R.id.browse_btn) try {
	Intent galleryIntent = new Intent(Intent.ACTION_PICK, MediaStore.Images.Media.EXTERNAL_CONTENT_URI);
	startActivityForResult(galleryIntent, RESULT_GALLERY);
} catch (ActivityNotFoundException anfe) {...}
		\end{java}
	\subsection{Enviament de dades al servidor}
		Per poder enviar dades al servidor, serà necessari l'ús d'una connexió a Internet. Per tant, s'haurà d'habilitar el permís necessari.\\\\
		Inicialment s'havia pensat d'enviar la imatge seleccionada, codificada en base 64, però només funcionava amb imatges molt petites. Per tant, s'ha considerat millor opció utilitzar alguna biblioteca
		d'Android com Volley o Retrofit.
